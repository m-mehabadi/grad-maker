{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Testing-GradientMaker.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-mehabadi/grad-maker/blob/main/_notebooks/Testing_GradientMaker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "http://www.cs.cmu.edu/~pradeepr/convexopt/Lecture_Slides/dual-ascent.pdf"
      ],
      "metadata": {
        "id": "BphVoTyaLF5t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WIHWSu69D0a-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# import torch\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# gd = np.array([\n",
        "#                [-24, 4],\n",
        "#                [3, -10],\n",
        "# ])\n",
        "\n",
        "# gd = np.random.randn(2, 2)\n",
        "\n",
        "# gd = np.array([\n",
        "#                [-0.3131, -0.2785],\n",
        "#                [ 2.1073,  1.0965]\n",
        "#                ])"
      ],
      "metadata": {
        "id": "9OmMsds-D5z3"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "BFFwqmMgwllu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def original_gradmaker(grads):\n",
        "#     alpha = 0.01\n",
        "#     epsilon = 0.5\n",
        "#     print(\"results for the ORIGINAL gradmaker\")\n",
        "#     gd = grads\n",
        "\n",
        "#     g = np.zeros(2) # the same as the dimension of the space\n",
        "#     u_ = np.zeros(2) # the same as the number of domains\n",
        "\n",
        "#     for i in range(20000):\n",
        "#         u_ = u_+alpha*(epsilon-gd@g)\n",
        "#         g = 1/2*np.sum(((1+(u_>=0)*u_).reshape(2, 1))*gd, axis=0)\n",
        "\n",
        "#     print(f'inner product with first vector: {np.dot(g, gd[0])}')\n",
        "#     print(f'inner product with second vector: {np.dot(g, gd[1])}')\n",
        "\n",
        "#     # plot vectors\n",
        "#     plt.plot([0, gd[0,0]], [0, gd[0, 1]], 'go-' , label='g1')\n",
        "#     plt.plot([0, gd[1,0]], [0, gd[1, 1]], 'ro-',label='g2')\n",
        "#     plt.plot([0,g[0]], [0,g[1]] ,'bo-' ,label='g*')\n",
        "#     leg = plt.legend(loc='upper center')\n",
        "#     plt.show()\n",
        "\n",
        "# def new_gradmaker(grads):\n",
        "#     print(\"results for the NEW gradmaker\")\n",
        "#     gd = grads\n",
        "#     g = gradient_maker(grads)\n",
        "#     print(f'inner product with first vector: {np.dot(g, gd[0])}')\n",
        "#     print(f'inner product with second vector: {np.dot(g, gd[1])}')\n",
        "\n",
        "#     # plot vectors\n",
        "#     plt.plot([0, gd[0,0]], [0, gd[0, 1]], 'go-' , label='g1')\n",
        "#     plt.plot([0, gd[1,0]], [0, gd[1, 1]], 'ro-',label='g2')\n",
        "#     plt.plot([0,g[0]], [0,g[1]] ,'bo-' ,label='g*')\n",
        "#     leg = plt.legend(loc='upper center')\n",
        "#     plt.show()"
      ],
      "metadata": {
        "id": "e2PxcLufw0S7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectors = np.random.randn(2, 2)"
      ],
      "metadata": {
        "id": "kZiI1htP5ady"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f'the main vectors:\\r\\n {vectors}')\n",
        "# original_gradmaker(vectors)\n",
        "# new_gradmaker(vectors)"
      ],
      "metadata": {
        "id": "-N4F1xd0FRy7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_maker(domain_grads, epsilon=0.5, alpha=0.01):\n",
        "    #### TODO: domain_grads must be a tensor itself.\n",
        " \n",
        "    # dgr = torch.stack((domain_grads))\n",
        "    dgr = domain_grads\n",
        "\n",
        "    # alpha = 0.01\n",
        "    # # epsilon = 0.5\n",
        "    # epsilon = 0.5\n",
        "    \n",
        "\n",
        "    number_of_domains, dim = dgr.shape\n",
        "\n",
        "    # g = torch.zeros(dim).to(device).numpy()\n",
        "    # u_ = torch.zeros(number_of_domains).to(device).numpy()\n",
        "    g = np.zeros(dim) #.to(device).numpy()\n",
        "    u_ = np.zeros(number_of_domains) #.to(device).numpy()\n",
        "    # prev_g = g\n",
        "    # current_g = torch.ones(dim).to(device).numpy()\n",
        "    # it = 0\n",
        "    # while True :\n",
        "    #     if np.sum((current_g-prev_g)**2).item()<=0.0001:\n",
        "    #         break\n",
        "    while not np.abs(np.min(dgr@g)-epsilon)<=0.001:\n",
        "    # for i in range(20000):\n",
        "        # prev_g = current_g\n",
        "        u_ = u_ + alpha*(epsilon - (dgr@g))\n",
        "        # u_ = u_ + alpha*(epsilon - (dgr@g))*(1+1./np.abs(epsilon - (dgr@g)))*(np.abs(epsilon - (dgr@g))>0.001) # this line is to make it converge faster, but didn't work :) don't use.\n",
        "        g = (1./number_of_domains)*np.sum(((1+(u_>=0)*u_).reshape(number_of_domains, 1))*dgr, axis=0)\n",
        "        # current_g = g\n",
        "        # it = it+1\n",
        "    # print(\"it {}\".format(it))    \n",
        "    # print(\"Done.\")\n",
        "    return g"
      ],
      "metadata": {
        "id": "8-FeagOWwo97"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testcases\n",
        "\n",
        "Now let's write some test cases to make sure everything is working correctly\n"
      ],
      "metadata": {
        "id": "PWTyzUQQ5zrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_num_of_domains = np.random.randint(2,1000)\n",
        "# test_dim = np.random.randint(test_num_of_domains, int((1+np.random.rand(1).item())*test_num_of_domains))\n",
        "# test_grads = np.random.randn(test_num_of_domains, test_dim)\n",
        "# g = gradient_maker(test_grads, epsilon, alpha)\n",
        "# assert np.abs(np.min(test_grads@g)-epsilon) <= 0.001, \"Somewhere is wrong!\"\n",
        "# print(f\"in {test_num_of_domains} domains with {test_dim} dimensions a candidate was found!\")\n",
        "# print(f\"the minimum inner product is: {np.min(test_grads@g)}\")"
      ],
      "metadata": {
        "id": "M8jQAapxy8Wz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# g = gradient_maker(test_grads, epsilon, alpha)\n",
        "# assert np.abs(np.min(test_grads@g)-epsilon) <= 0.001, \"Somewhere is wrong!\"\n",
        "# print(f\"in {test_num_of_domains} domains with {test_dim} dimensions a candidate was found!\")\n",
        "# print(f\"the minimum inner product is: {np.min(test_grads@g)}\")"
      ],
      "metadata": {
        "id": "DszTjRO5-2DI"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon=0.1\n",
        "alpha=0.05"
      ],
      "metadata": {
        "id": "FpmeOVAY7GuJ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grads = np.random.randn(2, 2000)"
      ],
      "metadata": {
        "id": "C1ZjLcuk-8WO"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grads.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lOiWyjAHlO5",
        "outputId": "12c0dcab-b38c-46f6-da13-4924c067cb83"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 2000)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = gradient_maker(grads, epsilon=epsilon, alpha=alpha)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "0cT4jdRbHl7Y",
        "outputId": "6674c414-f483-4316-ffca-9f3d106d5507"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-90f58e4209dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_maker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-9335e0b1ad14>\u001b[0m in \u001b[0;36mgradient_maker\u001b[0;34m(domain_grads, epsilon, alpha)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#     if np.sum((current_g-prev_g)**2).item()<=0.0001:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#         break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdgr\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m# for i in range(20000):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# prev_g = current_g\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamin\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}